{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_models(model_paths):\n",
    "        \n",
    "    class DeepAutoencoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(DeepAutoencoder, self).__init__()\n",
    "            self.Encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 256 -> 128\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 128 -> 64\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 64 -> 32\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 32 -> 16\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 16 -> 8\n",
    "            )\n",
    "            self.Decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),  # 8 -> 16\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),  # 16 -> 32\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),  # 32 -> 64\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),  # 64 -> 128\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(16, 3, kernel_size=2, stride=2),  # 128 -> 256\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.Encoder(x)\n",
    "            x = self.Decoder(x)\n",
    "            return x\n",
    "    \n",
    "    models = []\n",
    "    for model_path in model_paths:\n",
    "        model = DeepAutoencoder().cuda()\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像分割サイズ\n",
    "distance = 224\n",
    "\n",
    "# 画像を分割する関数\n",
    "def split(FILES):\n",
    "    # 分割後の画像を分割前の画像ごとに格納\n",
    "    split_images = []\n",
    "    for i in range(len(FILES)):\n",
    "        file = FILES[i]  # ファイル名\n",
    "        img = Image.open(file)  # 画像読み込み\n",
    "        img = np.array(img)  # Pillowの画像をnumpy配列に変換\n",
    "        h, w = img.shape[:2]  # 画像のサイズ\n",
    "        # 分割の始点\n",
    "        cx = 0\n",
    "        cy = 0\n",
    "        for x in range(h // distance):\n",
    "            for y in range(w // distance):\n",
    "                # 画像の切り取り\n",
    "                split_img = img[cx:cx + distance, cy:cy + distance]\n",
    "                # 画像の格納\n",
    "                split_images.append(Image.fromarray(split_img))  # numpy配列をPillowの画像に変換して格納\n",
    "                cy += distance\n",
    "            cy = 0\n",
    "            cx += distance\n",
    "    return split_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE(IMGS, model_paths, threshold, noise_kernel_size=5, morph_kernel_size=5, use_morphology=False, normalize=False):\n",
    "    models = make_models(model_paths)\n",
    "    prepocess = T.Compose([T.ToTensor()])\n",
    "    model_names = [os.path.splitext(os.path.basename(path))[0] for path in model_paths]\n",
    "    # 一番最後の_の次のアルファベットを取得\n",
    "    last_chars = [name.split('_')[-1][0] for name in model_names]\n",
    "\n",
    "    # result_dirを作成\n",
    "    result_dir = f'imgs/main_{\"\".join(last_chars)}_search_result'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "    # 各サブフォルダを作成\n",
    "    suffix = f\"noise_kernel_{noise_kernel_size}\"\n",
    "    if use_morphology:\n",
    "        suffix += f\"_morph_kernel_{morph_kernel_size}\"\n",
    "    else:\n",
    "        suffix += \"_no_morph\"\n",
    "    if normalize:\n",
    "        suffix += \"_norm\"\n",
    "    else:\n",
    "        suffix += \"_nonorm\"\n",
    "\n",
    "    diff_img_dir = os.path.join(result_dir, 'new_diff_img')\n",
    "    binary_img_dir = os.path.join(result_dir, f'{threshold}_new_binary_img_{suffix}')\n",
    "    output_img_dir = os.path.join(result_dir, f'new_output_img')\n",
    "    input_img_dir = os.path.join(result_dir, 'new_input_img')\n",
    "    gray_diff_img_dir = os.path.join(result_dir, 'new_gray_diff_img')\n",
    "    norm_diff_img_dir = os.path.join(result_dir, 'new_norm_diff_img')\n",
    "    contour_img_dir = os.path.join(result_dir, f'{threshold}_new_contour_img_{suffix}')\n",
    "    combined_img_dir = os.path.join(result_dir, f'{threshold}_combined_img_{suffix}')\n",
    "\n",
    "    if not os.path.exists(diff_img_dir):\n",
    "        os.makedirs(diff_img_dir)\n",
    "    if not os.path.exists(contour_img_dir):\n",
    "        os.makedirs(contour_img_dir)\n",
    "    if not os.path.exists(binary_img_dir):\n",
    "        os.makedirs(binary_img_dir)\n",
    "    if not os.path.exists(output_img_dir):\n",
    "        os.makedirs(output_img_dir)\n",
    "    if not os.path.exists(input_img_dir):\n",
    "        os.makedirs(input_img_dir)\n",
    "    if not os.path.exists(combined_img_dir):\n",
    "        os.makedirs(combined_img_dir)\n",
    "    if not os.path.exists(gray_diff_img_dir):\n",
    "        os.makedirs(gray_diff_img_dir)\n",
    "    if not os.path.exists(norm_diff_img_dir):\n",
    "        os.makedirs(norm_diff_img_dir)\n",
    "\n",
    "    for img_idx, IMG in enumerate(IMGS):\n",
    "        diff_list = []\n",
    "        max_areas = []\n",
    "        for model_idx, model in enumerate(models):\n",
    "            model.eval()\n",
    "            model_diff_dir = os.path.join(diff_img_dir, model_names[model_idx])  # モデルの順番をフォルダ名に使用\n",
    "            model_contour_dir = os.path.join(contour_img_dir, model_names[model_idx])  # 輪郭画像の保存先フォルダ\n",
    "            model_binary_dir = os.path.join(binary_img_dir, model_names[model_idx])  # 二値化画像の保存先フォルダ\n",
    "            model_output_dir = os.path.join(output_img_dir, model_names[model_idx])  # output画像の保存先フォルダ\n",
    "            model_input_dir = os.path.join(input_img_dir, model_names[model_idx])  # input画像の保存先フォルダ\n",
    "            model_gray_diff_dir = os.path.join(gray_diff_img_dir, model_names[model_idx])  # グレースケールdiff画像の保存先フォルダ\n",
    "            model_norm_diff_dir = os.path.join(norm_diff_img_dir, model_names[model_idx])  # 正規化diff画像の保存先フォルダ\n",
    "\n",
    "            # モデル順のフォルダを作成\n",
    "            if not os.path.exists(model_diff_dir):\n",
    "                os.makedirs(model_diff_dir)\n",
    "            if not os.path.exists(model_contour_dir):\n",
    "                os.makedirs(model_contour_dir)\n",
    "            if not os.path.exists(model_binary_dir):\n",
    "                os.makedirs(model_binary_dir)\n",
    "            if not os.path.exists(model_output_dir):\n",
    "                os.makedirs(model_output_dir)\n",
    "            if not os.path.exists(model_input_dir):\n",
    "                os.makedirs(model_input_dir)\n",
    "            if not os.path.exists(model_gray_diff_dir):\n",
    "                os.makedirs(model_gray_diff_dir)\n",
    "            if not os.path.exists(model_norm_diff_dir):\n",
    "                os.makedirs(model_norm_diff_dir)\n",
    "\n",
    "            img_tensor = prepocess(IMG).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                output = model(img_tensor)[0]\n",
    "            output = output.cpu().numpy().transpose(1, 2, 0)\n",
    "            output = np.uint8(np.maximum(np.minimum(output * 255, 255), 0))\n",
    "            origin = np.uint8(img_tensor[0].cpu().numpy().transpose(1, 2, 0) * 255)\n",
    "            diff = np.uint8(np.sqrt((output.astype(np.float32) - origin.astype(np.float32)) ** 2))\n",
    "            diff_mse = np.uint8((output.astype(np.float32) - origin.astype(np.float32)) ** 2)\n",
    "            diff_mse = np.sum(diff_mse)\n",
    "            diff_mse_sum = diff_mse / (224 * 224 * 3)\n",
    "\n",
    "            # 画像の保存\n",
    "            img_counter = img_idx + 1  # 順番をカウンターとして使用\n",
    "            diff_path = os.path.join(model_diff_dir, f\"{diff_mse_sum}_{img_counter}_{model_names[model_idx]}.png\")\n",
    "            output_path = os.path.join(model_output_dir, f\"{img_counter}_{model_names[model_idx]}.png\")\n",
    "            binary_path = os.path.join(model_binary_dir, f\"{img_counter}_{model_names[model_idx]}.png\")\n",
    "            contour_img_path = os.path.join(model_contour_dir, f\"{img_counter}_{model_names[model_idx]}.png\")\n",
    "            input_path = os.path.join(model_input_dir, f\"{img_counter}.png\")\n",
    "            gray_diff_path = os.path.join(model_gray_diff_dir, f\"{img_counter}_{model_names[model_idx]}.png\")\n",
    "            norm_diff_path = os.path.join(model_norm_diff_dir, f\"{img_counter}_{model_names[model_idx]}.png\")\n",
    "\n",
    "            cv2.imwrite(diff_path, diff)\n",
    "\n",
    "            # output画像を保存（RGBからBGRに変換）\n",
    "            output_img = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(output_path, output_img)\n",
    "\n",
    "            # 入力画像を保存（RGBからBGRに変換）\n",
    "            input_img = cv2.cvtColor(np.array(IMG), cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(input_path, input_img)\n",
    "\n",
    "            # 差分画像をグレースケールに変換\n",
    "            gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # グレースケールdiff画像を保存\n",
    "            cv2.imwrite(gray_diff_path, gray_diff)\n",
    "\n",
    "            # 差分画像の値の範囲を確認\n",
    "            min_val, max_val, _, _ = cv2.minMaxLoc(gray_diff)\n",
    "            # print(f\"Diff image min value: {min_val}, max value: {max_val}\")\n",
    "\n",
    "            # 差分画像を正規化するかどうか\n",
    "            if normalize:\n",
    "                norm_diff = cv2.normalize(gray_diff, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            else:\n",
    "                norm_diff = gray_diff\n",
    "\n",
    "            # 正規化diff画像を保存\n",
    "            cv2.imwrite(norm_diff_path, norm_diff)\n",
    "\n",
    "            # 二値化\n",
    "            _, binary = cv2.threshold(norm_diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # 二値化画像を保存\n",
    "            cv2.imwrite(binary_path, binary)\n",
    "\n",
    "            # ノイズ除去のカーネル\n",
    "            noise_kernel = np.ones((noise_kernel_size, noise_kernel_size), np.uint8)\n",
    "            bin_img = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, noise_kernel)\n",
    "\n",
    "            if use_morphology:\n",
    "                # 輪郭抽出(拡張収縮を行う場合)\n",
    "                morph_kernel = np.ones((morph_kernel_size, morph_kernel_size), np.uint8)  # カーネルサイズを大きくして輪郭を補完\n",
    "                bin_img = cv2.dilate(bin_img, morph_kernel, iterations=2)  # 膨張処理を追加\n",
    "                bin_img = cv2.erode(bin_img, morph_kernel, iterations=2)  # 収縮処理を追加\n",
    "\n",
    "            # 輪郭抽出\n",
    "            contours, _ = cv2.findContours(bin_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contours = sorted(contours, key=cv2.contourArea, reverse=True)  # 面積順にソート\n",
    "\n",
    "            # 最大の物体の面積を取得\n",
    "            max_area = cv2.contourArea(contours[0]) if contours else 0\n",
    "            max_areas.append(max_area)\n",
    "\n",
    "            # 輪郭を塗りつぶし\n",
    "            contour_img = np.zeros((diff.shape[0], diff.shape[1], 3), dtype=np.uint8)  # 正しい形式で初期化\n",
    "            for contour in contours:\n",
    "                color = [random.randint(0, 255) for _ in range(3)]  # ランダムな色を生成\n",
    "                cv2.drawContours(contour_img, [contour], -1, color, cv2.FILLED)  # ランダムな色で輪郭を塗りつぶし\n",
    "\n",
    "            # 輪郭画像を保存\n",
    "            contour_img_path = os.path.join(model_contour_dir, f\"{max_area:.2f}_{img_counter}_{model_names[model_idx]}.png\")\n",
    "            cv2.imwrite(contour_img_path, contour_img)\n",
    "\n",
    "\n",
    "        # 入力画像と対応するモデルの数分の輪郭画像を横に並べる\n",
    "        input_img = Image.open(input_path)\n",
    "        combined_width = input_img.width * (len(models) + 1)\n",
    "        combined_height = input_img.height\n",
    "        combined_img = Image.new('RGB', (combined_width, combined_height))\n",
    "\n",
    "        # 入力画像を左端に配置\n",
    "        combined_img.paste(input_img, (0, 0))\n",
    "\n",
    "        # 輪郭画像を順に配置\n",
    "        for model_idx in range(len(models)):\n",
    "            contour_img_path = os.path.join(contour_img_dir, model_names[model_idx], f\"{max_areas[model_idx]:.2f}_{img_counter}_{model_names[model_idx]}.png\")\n",
    "            contour_img = Image.open(contour_img_path)\n",
    "            combined_img.paste(contour_img, (input_img.width * (model_idx + 1), 0))\n",
    "\n",
    "            # 画像の上にモデル番号、最大面積、ノイズ除去カーネルサイズ、二値化閾値、正規化の有無を明記\n",
    "            draw = ImageDraw.Draw(combined_img)\n",
    "            font = ImageFont.load_default()\n",
    "            text_position = (input_img.width * (model_idx + 1) + 10, 10)\n",
    "            draw.text(text_position, f\"{model_names[model_idx]}\", fill=(255, 255, 255), font=font)\n",
    "            text_position = (input_img.width * (model_idx + 1) + 10, 30)\n",
    "            draw.text(text_position, f\"Max Area: {max_areas[model_idx]:.2f}\", fill=(255, 255, 255), font=font)\n",
    "            text_position = (input_img.width * (model_idx + 1) + 10, 50)\n",
    "            draw.text(text_position, f\"Noise Kernel Size: {noise_kernel_size}\", fill=(255, 255, 255), font=font)\n",
    "            if use_morphology:\n",
    "                text_position = (input_img.width * (model_idx + 1) + 10, 70)\n",
    "                draw.text(text_position, f\"Morph Kernel Size: {morph_kernel_size}\", fill=(255, 255, 255), font=font)\n",
    "            text_position = (input_img.width * (model_idx + 1) + 10, 90)\n",
    "            draw.text(text_position, f\"Threshold: {threshold}\", fill=(255, 255, 255), font=font)\n",
    "            text_position = (input_img.width * (model_idx + 1) + 10, 110)\n",
    "            draw.text(text_position, f\"Normalize: {'Yes' if normalize else 'No'}\", fill=(255, 255, 255), font=font)\n",
    "\n",
    "        # 結合画像を保存\n",
    "        combined_img_path = os.path.join(combined_img_dir, f\"{img_counter}.png\")\n",
    "        combined_img.save(combined_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1273268/309347958.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "d_model_paths = [\"models/fine_model_paths/6048_fineAEdeepmodel_20241226_ddark.pth\",\"models/fine_model_paths/6048_fineAEdeepmodel_20241226_dlight.pth\",\"models/fine_model_paths/6048_fineAEdeepmodel_20241226_dwhite.pth\"]\n",
    "r_model_paths = [\"models/fine_model_paths/6048_fineAEdeepmodel_20241226_rdark.pth\",\"models/fine_model_paths/6048_fineAEdeepmodel_20241226_rlight.pth\",\"models/fine_model_paths/6048_fineAEdeepmodel_20241226_rwhite.pth\"]\n",
    "threshold = 30\n",
    "files = list(glob.glob(\"imgs/test_img/*.JPG\"))\n",
    "split_images = split(files)\n",
    "AE(split_images, d_model_paths, threshold, noise_kernel_size=3, morph_kernel_size=0, use_morphology=False, normalize=False)\n",
    "AE(split_images, r_model_paths, threshold, noise_kernel_size=3, morph_kernel_size=0, use_morphology=False, normalize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
